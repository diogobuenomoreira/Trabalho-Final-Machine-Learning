{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.callbacks import TensorBoard\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    #get train data\n",
    "    train_data_path ='train.csv'\n",
    "    train = pd.read_csv(train_data_path)\n",
    "    \n",
    "    #get test data\n",
    "    seed = 7\n",
    "    np.random.seed(seed)\n",
    "    train, validation = train_test_split(train, test_size=0.2, random_state=seed)    \n",
    "    return train,validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train , validation = get_data()\n",
    "train_target = train.Servidas\n",
    "train.drop(['Servidas'],axis = 1 , inplace = True)\n",
    "validation_target=validation.Servidas\n",
    "validation.drop(['Servidas'],axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input layer\n",
    "NN_model.add(Dense(129, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hidden layer\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Out Layer\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 129)               16770     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               33280     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 181,891\n",
      "Trainable params: 181,891\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile layer\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2578/2578 [==============================] - 0s 83us/step - loss: 220.0828 - mean_absolute_error: 220.0828\n",
      "Epoch 2/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 216.3902 - mean_absolute_error: 216.3902\n",
      "Epoch 3/200\n",
      "2578/2578 [==============================] - 0s 83us/step - loss: 196.4975 - mean_absolute_error: 196.4975\n",
      "Epoch 4/200\n",
      "2578/2578 [==============================] - 0s 83us/step - loss: 201.0235 - mean_absolute_error: 201.0235\n",
      "Epoch 5/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 229.6266 - mean_absolute_error: 229.6266\n",
      "Epoch 6/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 222.8502 - mean_absolute_error: 222.8502\n",
      "Epoch 7/200\n",
      "2578/2578 [==============================] - 0s 82us/step - loss: 212.9985 - mean_absolute_error: 212.9985\n",
      "Epoch 8/200\n",
      "2578/2578 [==============================] - 0s 83us/step - loss: 213.6662 - mean_absolute_error: 213.6662\n",
      "Epoch 9/200\n",
      "2578/2578 [==============================] - 0s 83us/step - loss: 202.9711 - mean_absolute_error: 202.9711\n",
      "Epoch 10/200\n",
      "2578/2578 [==============================] - 0s 83us/step - loss: 204.2192 - mean_absolute_error: 204.2192\n",
      "Epoch 11/200\n",
      "2578/2578 [==============================] - 0s 82us/step - loss: 223.1241 - mean_absolute_error: 223.1241\n",
      "Epoch 12/200\n",
      "2578/2578 [==============================] - 0s 82us/step - loss: 212.5040 - mean_absolute_error: 212.5040\n",
      "Epoch 13/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 210.9723 - mean_absolute_error: 210.9723\n",
      "Epoch 14/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 198.7167 - mean_absolute_error: 198.7167\n",
      "Epoch 15/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 199.9009 - mean_absolute_error: 199.9009\n",
      "Epoch 16/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 203.5046 - mean_absolute_error: 203.5046\n",
      "Epoch 17/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 205.4035 - mean_absolute_error: 205.4035\n",
      "Epoch 18/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 193.9175 - mean_absolute_error: 193.9175\n",
      "Epoch 19/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 214.1756 - mean_absolute_error: 214.1756\n",
      "Epoch 20/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 225.1032 - mean_absolute_error: 225.1032\n",
      "Epoch 21/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 208.2172 - mean_absolute_error: 208.2172\n",
      "Epoch 22/200\n",
      "2578/2578 [==============================] - 0s 82us/step - loss: 193.5733 - mean_absolute_error: 193.5733\n",
      "Epoch 23/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 201.2004 - mean_absolute_error: 201.2004\n",
      "Epoch 24/200\n",
      "2578/2578 [==============================] - 0s 77us/step - loss: 190.9271 - mean_absolute_error: 190.9271\n",
      "Epoch 25/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 209.0568 - mean_absolute_error: 209.0568\n",
      "Epoch 26/200\n",
      "2578/2578 [==============================] - 0s 82us/step - loss: 202.1940 - mean_absolute_error: 202.1940\n",
      "Epoch 27/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 197.5192 - mean_absolute_error: 197.5192\n",
      "Epoch 28/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 213.1545 - mean_absolute_error: 213.1545\n",
      "Epoch 29/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 190.8052 - mean_absolute_error: 190.8052\n",
      "Epoch 30/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 194.8759 - mean_absolute_error: 194.8759\n",
      "Epoch 31/200\n",
      "2578/2578 [==============================] - 0s 83us/step - loss: 201.3117 - mean_absolute_error: 201.3117\n",
      "Epoch 32/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 204.0313 - mean_absolute_error: 204.0313\n",
      "Epoch 33/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 207.0254 - mean_absolute_error: 207.0254\n",
      "Epoch 34/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 192.1553 - mean_absolute_error: 192.1553\n",
      "Epoch 35/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 196.8919 - mean_absolute_error: 196.8919\n",
      "Epoch 36/200\n",
      "2578/2578 [==============================] - 0s 82us/step - loss: 202.0877 - mean_absolute_error: 202.0877\n",
      "Epoch 37/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 196.1225 - mean_absolute_error: 196.1225\n",
      "Epoch 38/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 190.4998 - mean_absolute_error: 190.4998\n",
      "Epoch 39/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 178.8221 - mean_absolute_error: 178.8221\n",
      "Epoch 40/200\n",
      "2578/2578 [==============================] - 0s 83us/step - loss: 183.5215 - mean_absolute_error: 183.5215\n",
      "Epoch 41/200\n",
      "2578/2578 [==============================] - 0s 76us/step - loss: 188.9168 - mean_absolute_error: 188.9168\n",
      "Epoch 42/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 191.6655 - mean_absolute_error: 191.6655\n",
      "Epoch 43/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 184.1049 - mean_absolute_error: 184.1049\n",
      "Epoch 44/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 179.2144 - mean_absolute_error: 179.2144\n",
      "Epoch 45/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 194.2858 - mean_absolute_error: 194.2858\n",
      "Epoch 46/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 215.3729 - mean_absolute_error: 215.3729\n",
      "Epoch 47/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 183.3206 - mean_absolute_error: 183.3206\n",
      "Epoch 48/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 197.3902 - mean_absolute_error: 197.3902\n",
      "Epoch 49/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 182.3234 - mean_absolute_error: 182.3234\n",
      "Epoch 50/200\n",
      "2578/2578 [==============================] - 0s 84us/step - loss: 177.0976 - mean_absolute_error: 177.0976\n",
      "Epoch 51/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 209.1172 - mean_absolute_error: 209.1172\n",
      "Epoch 52/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 187.3029 - mean_absolute_error: 187.3029\n",
      "Epoch 53/200\n",
      "2578/2578 [==============================] - 0s 83us/step - loss: 189.3843 - mean_absolute_error: 189.3843\n",
      "Epoch 54/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 180.0721 - mean_absolute_error: 180.0721\n",
      "Epoch 55/200\n",
      "2578/2578 [==============================] - 0s 82us/step - loss: 181.1754 - mean_absolute_error: 181.1754\n",
      "Epoch 56/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 209.8588 - mean_absolute_error: 209.8588\n",
      "Epoch 57/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 175.1041 - mean_absolute_error: 175.1041\n",
      "Epoch 58/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 188.4447 - mean_absolute_error: 188.4447\n",
      "Epoch 59/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 189.2282 - mean_absolute_error: 189.2282\n",
      "Epoch 60/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 182.3669 - mean_absolute_error: 182.3669\n",
      "Epoch 61/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 204.7923 - mean_absolute_error: 204.7923\n",
      "Epoch 62/200\n",
      "2578/2578 [==============================] - 0s 77us/step - loss: 194.0409 - mean_absolute_error: 194.0409\n",
      "Epoch 63/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 176.7943 - mean_absolute_error: 176.7943\n",
      "Epoch 64/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 177.6133 - mean_absolute_error: 177.6133\n",
      "Epoch 65/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 196.0131 - mean_absolute_error: 196.0131\n",
      "Epoch 66/200\n",
      "2578/2578 [==============================] - 0s 84us/step - loss: 177.0167 - mean_absolute_error: 177.0167\n",
      "Epoch 67/200\n",
      "2578/2578 [==============================] - 0s 77us/step - loss: 169.4972 - mean_absolute_error: 169.4972\n",
      "Epoch 68/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 186.0753 - mean_absolute_error: 186.0753\n",
      "Epoch 69/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2578/2578 [==============================] - 0s 84us/step - loss: 187.3298 - mean_absolute_error: 187.3298\n",
      "Epoch 70/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 197.7898 - mean_absolute_error: 197.7898\n",
      "Epoch 71/200\n",
      "2578/2578 [==============================] - 0s 83us/step - loss: 192.6612 - mean_absolute_error: 192.6612\n",
      "Epoch 72/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 179.5184 - mean_absolute_error: 179.5184\n",
      "Epoch 73/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 164.6943 - mean_absolute_error: 164.6943\n",
      "Epoch 74/200\n",
      "2578/2578 [==============================] - 0s 85us/step - loss: 208.9315 - mean_absolute_error: 208.9315\n",
      "Epoch 75/200\n",
      "2578/2578 [==============================] - 0s 77us/step - loss: 181.6505 - mean_absolute_error: 181.6505\n",
      "Epoch 76/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 175.8934 - mean_absolute_error: 175.8934\n",
      "Epoch 77/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 170.1296 - mean_absolute_error: 170.1296\n",
      "Epoch 78/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 189.0667 - mean_absolute_error: 189.0667\n",
      "Epoch 79/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 180.5212 - mean_absolute_error: 180.5212\n",
      "Epoch 80/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 170.6241 - mean_absolute_error: 170.6241\n",
      "Epoch 81/200\n",
      "2578/2578 [==============================] - 0s 83us/step - loss: 176.4648 - mean_absolute_error: 176.4648\n",
      "Epoch 82/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 180.6024 - mean_absolute_error: 180.6024\n",
      "Epoch 83/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 172.5635 - mean_absolute_error: 172.5635\n",
      "Epoch 84/200\n",
      "2578/2578 [==============================] - 0s 84us/step - loss: 178.1124 - mean_absolute_error: 178.1124\n",
      "Epoch 85/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 167.9125 - mean_absolute_error: 167.9125\n",
      "Epoch 86/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 178.3651 - mean_absolute_error: 178.3651\n",
      "Epoch 87/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 184.4829 - mean_absolute_error: 184.4829\n",
      "Epoch 88/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 164.2244 - mean_absolute_error: 164.2244\n",
      "Epoch 89/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 164.7730 - mean_absolute_error: 164.7730\n",
      "Epoch 90/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 173.7703 - mean_absolute_error: 173.7703\n",
      "Epoch 91/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 171.1683 - mean_absolute_error: 171.1683\n",
      "Epoch 92/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 178.5515 - mean_absolute_error: 178.5515\n",
      "Epoch 93/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 213.8800 - mean_absolute_error: 213.8800\n",
      "Epoch 94/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 193.3571 - mean_absolute_error: 193.3571\n",
      "Epoch 95/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 158.6606 - mean_absolute_error: 158.6606\n",
      "Epoch 96/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 160.6887 - mean_absolute_error: 160.6887\n",
      "Epoch 97/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 159.5176 - mean_absolute_error: 159.5176\n",
      "Epoch 98/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 176.5848 - mean_absolute_error: 176.5848\n",
      "Epoch 99/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 202.9354 - mean_absolute_error: 202.9354\n",
      "Epoch 100/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 189.2164 - mean_absolute_error: 189.2164\n",
      "Epoch 101/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 183.7592 - mean_absolute_error: 183.7592\n",
      "Epoch 102/200\n",
      "2578/2578 [==============================] - 0s 82us/step - loss: 173.8215 - mean_absolute_error: 173.8215\n",
      "Epoch 103/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 163.5388 - mean_absolute_error: 163.5388\n",
      "Epoch 104/200\n",
      "2578/2578 [==============================] - 0s 83us/step - loss: 173.9740 - mean_absolute_error: 173.9740\n",
      "Epoch 105/200\n",
      "2578/2578 [==============================] - 0s 76us/step - loss: 164.0186 - mean_absolute_error: 164.0186\n",
      "Epoch 106/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 163.6753 - mean_absolute_error: 163.6753\n",
      "Epoch 107/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 169.8460 - mean_absolute_error: 169.8460\n",
      "Epoch 108/200\n",
      "2578/2578 [==============================] - 0s 83us/step - loss: 171.3604 - mean_absolute_error: 171.3604\n",
      "Epoch 109/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 165.5021 - mean_absolute_error: 165.5021\n",
      "Epoch 110/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 173.3499 - mean_absolute_error: 173.3499\n",
      "Epoch 111/200\n",
      "2578/2578 [==============================] - 0s 77us/step - loss: 168.9994 - mean_absolute_error: 168.9994\n",
      "Epoch 112/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 164.0702 - mean_absolute_error: 164.0702\n",
      "Epoch 113/200\n",
      "2578/2578 [==============================] - 0s 83us/step - loss: 161.0347 - mean_absolute_error: 161.0347\n",
      "Epoch 114/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 173.9461 - mean_absolute_error: 173.9461\n",
      "Epoch 115/200\n",
      "2578/2578 [==============================] - 0s 82us/step - loss: 175.5829 - mean_absolute_error: 175.5829\n",
      "Epoch 116/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 164.8265 - mean_absolute_error: 164.8265\n",
      "Epoch 117/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 162.1063 - mean_absolute_error: 162.1063\n",
      "Epoch 118/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 158.3034 - mean_absolute_error: 158.3034\n",
      "Epoch 119/200\n",
      "2578/2578 [==============================] - 0s 76us/step - loss: 162.9199 - mean_absolute_error: 162.9199\n",
      "Epoch 120/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 181.6921 - mean_absolute_error: 181.6921\n",
      "Epoch 121/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 154.2923 - mean_absolute_error: 154.2923\n",
      "Epoch 122/200\n",
      "2578/2578 [==============================] - 0s 82us/step - loss: 172.3413 - mean_absolute_error: 172.3413\n",
      "Epoch 123/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 167.1183 - mean_absolute_error: 167.1183\n",
      "Epoch 124/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 171.9018 - mean_absolute_error: 171.9018\n",
      "Epoch 125/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 160.6410 - mean_absolute_error: 160.6410\n",
      "Epoch 126/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 146.3181 - mean_absolute_error: 146.3181\n",
      "Epoch 127/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 171.0861 - mean_absolute_error: 171.0861\n",
      "Epoch 128/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 168.1298 - mean_absolute_error: 168.1298\n",
      "Epoch 129/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 171.6907 - mean_absolute_error: 171.6907\n",
      "Epoch 130/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 171.7920 - mean_absolute_error: 171.7920\n",
      "Epoch 131/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 155.5613 - mean_absolute_error: 155.5613\n",
      "Epoch 132/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 164.4467 - mean_absolute_error: 164.4467\n",
      "Epoch 133/200\n",
      "2578/2578 [==============================] - 0s 83us/step - loss: 150.2919 - mean_absolute_error: 150.2919\n",
      "Epoch 134/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 172.8836 - mean_absolute_error: 172.8836\n",
      "Epoch 135/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 152.1379 - mean_absolute_error: 152.1379\n",
      "Epoch 136/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 145.4743 - mean_absolute_error: 145.4743\n",
      "Epoch 137/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2578/2578 [==============================] - 0s 83us/step - loss: 174.3369 - mean_absolute_error: 174.3369\n",
      "Epoch 138/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 168.3308 - mean_absolute_error: 168.3308\n",
      "Epoch 139/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 155.6168 - mean_absolute_error: 155.6168\n",
      "Epoch 140/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 159.5317 - mean_absolute_error: 159.5317\n",
      "Epoch 141/200\n",
      "2578/2578 [==============================] - 0s 82us/step - loss: 160.5720 - mean_absolute_error: 160.5720\n",
      "Epoch 142/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 170.8517 - mean_absolute_error: 170.8517\n",
      "Epoch 143/200\n",
      "2578/2578 [==============================] - 0s 82us/step - loss: 192.9145 - mean_absolute_error: 192.9145\n",
      "Epoch 144/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 168.9594 - mean_absolute_error: 168.9594\n",
      "Epoch 145/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 174.3482 - mean_absolute_error: 174.3482\n",
      "Epoch 146/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 170.7559 - mean_absolute_error: 170.7559\n",
      "Epoch 147/200\n",
      "2578/2578 [==============================] - 0s 84us/step - loss: 150.8921 - mean_absolute_error: 150.8921\n",
      "Epoch 148/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 145.6320 - mean_absolute_error: 145.6320\n",
      "Epoch 149/200\n",
      "2578/2578 [==============================] - 0s 77us/step - loss: 151.4809 - mean_absolute_error: 151.4809\n",
      "Epoch 150/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 145.8094 - mean_absolute_error: 145.8094\n",
      "Epoch 151/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 157.3814 - mean_absolute_error: 157.3814\n",
      "Epoch 152/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 155.7748 - mean_absolute_error: 155.7748\n",
      "Epoch 153/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 146.6140 - mean_absolute_error: 146.6140\n",
      "Epoch 154/200\n",
      "2578/2578 [==============================] - 0s 82us/step - loss: 172.3116 - mean_absolute_error: 172.3116\n",
      "Epoch 155/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 153.2422 - mean_absolute_error: 153.2422\n",
      "Epoch 156/200\n",
      "2578/2578 [==============================] - 0s 83us/step - loss: 151.9340 - mean_absolute_error: 151.9340\n",
      "Epoch 157/200\n",
      "2578/2578 [==============================] - 0s 84us/step - loss: 148.8835 - mean_absolute_error: 148.8835\n",
      "Epoch 158/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 142.8756 - mean_absolute_error: 142.8756\n",
      "Epoch 159/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 167.8274 - mean_absolute_error: 167.8274\n",
      "Epoch 160/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 164.1201 - mean_absolute_error: 164.1201\n",
      "Epoch 161/200\n",
      "2578/2578 [==============================] - 0s 84us/step - loss: 162.8881 - mean_absolute_error: 162.8881\n",
      "Epoch 162/200\n",
      "2578/2578 [==============================] - 0s 84us/step - loss: 157.5705 - mean_absolute_error: 157.5705\n",
      "Epoch 163/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 149.5626 - mean_absolute_error: 149.5626\n",
      "Epoch 164/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 156.3524 - mean_absolute_error: 156.3524\n",
      "Epoch 165/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 144.2268 - mean_absolute_error: 144.2268\n",
      "Epoch 166/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 144.8068 - mean_absolute_error: 144.8068\n",
      "Epoch 167/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 151.9974 - mean_absolute_error: 151.9974\n",
      "Epoch 168/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 165.2355 - mean_absolute_error: 165.2355\n",
      "Epoch 169/200\n",
      "2578/2578 [==============================] - 0s 82us/step - loss: 183.2814 - mean_absolute_error: 183.2814\n",
      "Epoch 170/200\n",
      "2578/2578 [==============================] - 0s 77us/step - loss: 162.7120 - mean_absolute_error: 162.7120\n",
      "Epoch 171/200\n",
      "2578/2578 [==============================] - 0s 87us/step - loss: 165.1143 - mean_absolute_error: 165.1143\n",
      "Epoch 172/200\n",
      "2578/2578 [==============================] - 0s 83us/step - loss: 153.8317 - mean_absolute_error: 153.8317\n",
      "Epoch 173/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 168.3755 - mean_absolute_error: 168.3755\n",
      "Epoch 174/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 163.7083 - mean_absolute_error: 163.7083\n",
      "Epoch 175/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 156.3891 - mean_absolute_error: 156.3891\n",
      "Epoch 176/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 133.4851 - mean_absolute_error: 133.4851\n",
      "Epoch 177/200\n",
      "2578/2578 [==============================] - 0s 77us/step - loss: 150.0906 - mean_absolute_error: 150.0906\n",
      "Epoch 178/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 155.7794 - mean_absolute_error: 155.7794\n",
      "Epoch 179/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 174.5309 - mean_absolute_error: 174.5309\n",
      "Epoch 180/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 145.6841 - mean_absolute_error: 145.6841\n",
      "Epoch 181/200\n",
      "2578/2578 [==============================] - 0s 84us/step - loss: 146.9533 - mean_absolute_error: 146.9533\n",
      "Epoch 182/200\n",
      "2578/2578 [==============================] - 0s 77us/step - loss: 178.3001 - mean_absolute_error: 178.3001\n",
      "Epoch 183/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 164.0279 - mean_absolute_error: 164.0279\n",
      "Epoch 184/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 151.3137 - mean_absolute_error: 151.3137\n",
      "Epoch 185/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 145.5909 - mean_absolute_error: 145.5909\n",
      "Epoch 186/200\n",
      "2578/2578 [==============================] - 0s 83us/step - loss: 166.4120 - mean_absolute_error: 166.4120\n",
      "Epoch 187/200\n",
      "2578/2578 [==============================] - 0s 78us/step - loss: 140.7600 - mean_absolute_error: 140.7600\n",
      "Epoch 188/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 156.8140 - mean_absolute_error: 156.8140\n",
      "Epoch 189/200\n",
      "2578/2578 [==============================] - 0s 83us/step - loss: 146.3111 - mean_absolute_error: 146.3111\n",
      "Epoch 190/200\n",
      "2578/2578 [==============================] - 0s 85us/step - loss: 156.8596 - mean_absolute_error: 156.8596\n",
      "Epoch 191/200\n",
      "2578/2578 [==============================] - 0s 84us/step - loss: 148.4826 - mean_absolute_error: 148.4826\n",
      "Epoch 192/200\n",
      "2578/2578 [==============================] - 0s 84us/step - loss: 153.3970 - mean_absolute_error: 153.3970\n",
      "Epoch 193/200\n",
      "2578/2578 [==============================] - 0s 81us/step - loss: 138.8590 - mean_absolute_error: 138.8590\n",
      "Epoch 194/200\n",
      "2578/2578 [==============================] - 0s 82us/step - loss: 157.3765 - mean_absolute_error: 157.3765\n",
      "Epoch 195/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 154.0937 - mean_absolute_error: 154.0937\n",
      "Epoch 196/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 138.5612 - mean_absolute_error: 138.5612\n",
      "Epoch 197/200\n",
      "2578/2578 [==============================] - 0s 79us/step - loss: 152.7961 - mean_absolute_error: 152.7961\n",
      "Epoch 198/200\n",
      "2578/2578 [==============================] - 0s 80us/step - loss: 165.6768 - mean_absolute_error: 165.6768\n",
      "Epoch 199/200\n",
      "2578/2578 [==============================] - 0s 83us/step - loss: 145.6560 - mean_absolute_error: 145.6560\n",
      "Epoch 200/200\n",
      "2578/2578 [==============================] - 0s 82us/step - loss: 136.4503 - mean_absolute_error: 136.4503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2685fbe80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbCallBack = TensorBoard(log_dir=\"/home/juliomb/alrac/Proyecto/logs\".format(time()), write_graph=True)\n",
    "NN_model.fit(train, train_target, epochs=200, batch_size=32,callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_r2_score(v_true, v_pred):\n",
    "    ssres = np.sum(np.square(v_true - v_pred))\n",
    "    sstot = np.sum(np.square(v_true - np.mean(v_true)))\n",
    "    return 1 - ssres / sstot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645/645 [==============================] - 0s 39us/step\n",
      "MAE =  482.591388933\n",
      "R2 =  0.911075849796\n"
     ]
    }
   ],
   "source": [
    "val_pre=NN_model.predict(validation,batch_size=32)\n",
    "score = NN_model.evaluate(validation, validation_target, batch_size=32)\n",
    "print('MAE = ', score[0])\n",
    "print('R2 = ', my_r2_score(validation_target.values.reshape((validation_target.shape[0],1)),val_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "validation_=validation.values[100:110,:]\n",
    "validation_target_=validation_target.values.reshape((validation_target.shape[0],1))[100:110,:]\n",
    "predict1_=val_pre[100:110,:]\n",
    "validation_reduced = PCA(n_components=1).fit_transform(validation_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A imagem mostra os dados verdadeiros em preto e em verde aqueles encontrados pela árvore de decisão\n",
    "plt.figure()\n",
    "plt.scatter(validation_reduced, validation_target_, c=\"k\", label=\"Training samples\")\n",
    "plt.scatter(validation_reduced,predict1_ , c=\"g\", label=\"Neural Network\")\n",
    "#plt.scatter(validation_reduced, predict2_, c=\"r\", label=\"n_estimators=300\")\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Neural Network Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
